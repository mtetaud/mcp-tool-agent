# MCP Tool Agent

**MCP Tool Agent** est un plugin TypeScript permettant d'exÃ©cuter dynamiquement des outils exposÃ©s par un  
serveur [MCP (Model Context Protocol)](https://modelcontextprotocol.io) Ã  partir d'une requÃªte utilisateur.  
Il est idÃ©al pour intÃ©grer des agents LangChain ou des chatbots avec des fonctionnalitÃ©s Ã©tendues, pilotÃ©es par un LLM.

---

## âœ¨ FonctionnalitÃ©s

- DÃ©couverte automatique des outils MCP disponibles
- IntÃ©gration avec LangChain (`DynamicStructuredTool`)
- Agent simplifiÃ© utilisant `RunnableSequence` (LLM + outil)
- Validation des entrÃ©es avec `zod`
- Facile Ã  intÃ©grer dans vos assistants ou serveurs LLM
- **Nouveau** : Support pour diffÃ©rents modÃ¨les LLM (OpenAI, Anthropic, Ollama)

---

## ğŸš€ Utilisation

**Exemple simple :**

```ts
import { handleMcpQuery } from 'mcp-tool-agent';

const response = await handleMcpQuery("Peux-tu utiliser l'outil getWeather pour connaÃ®tre la mÃ©tÃ©o Ã  Paris ?");
console.log(response);
```

**Avec configuration de LLM personnalisÃ©e :**

```ts
import { handleMcpQuery } from 'mcp-tool-agent';

// Avec Anthropic Claude
const response = await handleMcpQuery("Peux-tu utiliser l'outil getWeather pour connaÃ®tre la mÃ©tÃ©o Ã  Paris ?", {
  provider: 'anthropic',
  modelName: 'claude-3-sonnet-20240229',
  apiKey: 'votre-clÃ©-api-anthropic',
  temperature: 0.1
});

// Avec Ollama en local
const response = await handleMcpQuery("Peux-tu utiliser l'outil getWeather pour connaÃ®tre la mÃ©tÃ©o Ã  Paris ?", {
  provider: 'ollama',
  modelName: 'llama3',
  baseUrl: 'http://localhost:11434/v1',
  temperature: 0
});

console.log(response);
```

Ce code :

- dÃ©tecte les outils MCP disponibles
- utilise le LLM spÃ©cifiÃ© (ou GPT-4 par dÃ©faut)
- sÃ©lectionne le bon outil via le raisonnement du LLM
- exÃ©cute l'outil et formate la rÃ©ponse

---

## ğŸ“¦ Requirements

- Node.js v18+
- MCP CLI installÃ© :
  ```bash
  npm install -g @modelcontextprotocol/cli
  ```
- AccÃ¨s Ã  une API du LLM choisi :
  - OpenAI (par dÃ©faut)
  - Anthropic (pour Claude)
  - Ollama (pour les modÃ¨les en local)

---

## ğŸ§ª Tests et dÃ©veloppement local

```bash
npm run build
```

**Exemple de test :**

```ts
import { handleMcpQuery } from './dist/index.js';

const run = async () => {
  const res = await handleMcpQuery("Utilise l'outil 'hello' pour me saluer.");
  console.log(res);
};

run();
```

Consultez le dossier `examples` pour des exemples de configurations avec diffÃ©rents LLMs.

---

## ğŸ¤ Contribuer

1. Fork ce repo
2. CrÃ©e une branche
   ```bash
   git checkout -b feat/ma-fonction
   ```
3. Commit tes modifs
   ```bash
   git commit -am 'feat: ma fonction'
   ```
4. Push la branche
   ```bash
   git push origin feat/ma-fonction
   ```
5. Ouvre une Pull Request ğŸš€

---

## ğŸ“ Licence

MIT Â© mtetaud

---

## ğŸ”— Liens utiles

- ğŸŒ Site MCP : [modelcontextprotocol.io](https://modelcontextprotocol.io)
- ğŸ“¦ LangChain : [langchainjs.dev](https://www.langchainjs.dev)
- ğŸ’¬ GPT via LangChain : [`@langchain/openai`](https://www.npmjs.com/package/@langchain/openai)
- ğŸ¤– Claude via LangChain : [`@langchain/anthropic`](https://www.npmjs.com/package/@langchain/anthropic)
- ğŸ¦™ Ollama : [ollama.ai](https://ollama.ai)
